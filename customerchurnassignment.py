# -*- coding: utf-8 -*-
"""CustomerChurnAssignment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1neOiWPQ8edfJP2SgIKiI9ghdeJDuJlKO
"""

import pandas as pd
import numpy as np
from google.colab import drive
import tensorflow as tf

drive.mount('/content/drive')

customer_Churn = pd.read_csv('/content/drive/My Drive/Customer Churn/CustomerChurn_dataset.csv')

# Explore the dataset
print("Dataset Info:")
print(customer_Churn.info())

customer_Churn=customer_Churn.drop("customerID",axis=1)

customer_Churn

# Using EDA to visualise relationships
import seaborn as sns
import matplotlib.pyplot as plt

# Gender and Churn
sns.countplot(x='gender', hue='Churn', data=customer_Churn)
plt.title('Churn Distribution by Gender')
plt.show()

# Senior Citizens and Churn
sns.countplot(x='SeniorCitizen', hue='Churn', data=customer_Churn)
plt.title('Churn Distribution for Senior Citizens')
plt.show()

# Partner and Dependents
fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 5))
sns.countplot(x='Partner', hue='Churn', data=customer_Churn, ax=axes[0])
axes[0].set_title('Churn Distribution for Customers with Partners')

sns.countplot(x='Dependents', hue='Churn', data=customer_Churn, ax=axes[1])
axes[1].set_title('Churn Distribution for Customers with Dependents')
plt.show()

# Tenure and Churn
sns.boxplot(x='Churn', y='tenure', data=customer_Churn)
plt.title('Tenure Distribution for Churned and Non-Churned Customers')
plt.show()

# Multiple Features (e.g., Senior Citizen Men with Partners and Dependents)
filtered_data = customer_Churn[
    (customer_Churn['SeniorCitizen'] == 1) & (customer_Churn['gender'] == 'Male') &
    (customer_Churn['Partner'] == 'Yes') & (customer_Churn['Dependents'] == 'Yes')
]

if not filtered_data.empty:
    sns.countplot(x='Churn', data=filtered_data, color='skyblue')  # Set color explicitly
    plt.title('Churn Distribution for Senior Citizen Men with Partners and Dependents')
    plt.show()
else:
    print("No data matching the specified criteria.")

# Identify categorical columns by selecting columns with 'object' data type
categorical_columns = customer_Churn .select_dtypes(include=['object']).columns


from sklearn.preprocessing import LabelEncoder

# Initialize a LabelEncoder
label_encoder = LabelEncoder()

# Encode categorical columns in the combined dataset
for column in categorical_columns:
    customer_Churn[column] = label_encoder.fit_transform(customer_Churn[column])

customer_Churn

from sklearn.ensemble import RandomForestClassifier

#Here, Churn is the target variable
X= customer_Churn.drop('Churn',axis=1)
Y=customer_Churn['Churn']



#Splitting the data into training and testing sets
from sklearn.model_selection import train_test_split

X_train_rf, X_test_rf, y_train_rf, y_test_rf = train_test_split(X, Y, test_size=0.2, random_state=42)

# Initialize the Random Forest model
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)


# Fit the RandomForest model
rf_model.fit(X_train_rf, y_train_rf)

# Get feature importances
feature_importances_rf = rf_model.feature_importances_



# Create a DataFrame to display feature importances
feature_importance_df_rf = pd.DataFrame({'Feature': X_train_rf.columns, 'Importance': feature_importances_rf})

# Sort the DataFrame by importance in descending order
feature_importance_df_rf = feature_importance_df_rf.sort_values(by='Importance', ascending=False)

# Calculate cumulative importance
feature_importance_df_rf['Cumulative Importance'] = feature_importance_df_rf['Importance'].cumsum()

# Set the threshold
threshold_rf = 0.80

# Filter features based on the threshold
selected_features_rf = feature_importance_df_rf[feature_importance_df_rf['Cumulative Importance'] <= threshold_rf]['Feature']

# Display the selected features
print("\nSelected Features (Random Forest):")
print(selected_features_rf)

selected_features_rf

from sklearn.preprocessing import StandardScaler
# Apply selected features to both Random Forest and Keras models
X = X[selected_features_rf]
X_train_rf, X_test_rf, y_train_rf, y_test_rf = train_test_split(X, Y, test_size=0.2, random_state=42)


# Initialize the StandardScaler
scaler_keras = StandardScaler()


# Standardize numerical features for the Keras model
X = scaler_keras.fit_transform(X)



# Split data into training and testing sets for Keras
X_train_keras, X_test_keras, y_train_keras, y_test_keras = train_test_split(X, Y, test_size=0.2, random_state=42)

# Display the selected features
print("Selected Features:")
print(selected_features_rf)

# Convert X_train_keras to a DataFrame for display
X_train_df = pd.DataFrame(X_train_keras, columns=selected_features_rf)

# Display X_train_df
print("X_train_keras with selected features:")
print(X_train_df.head())

X

pip install scikeras

from tensorflow import keras
from scikeras.wrappers import KerasClassifier
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.metrics import accuracy_score
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense
from sklearn.preprocessing import StandardScaler, LabelEncoder



# Keras model creation function
def create_mlp_model(input_shape):
    inputs = Input(shape=(input_shape,))
    hidden1 = Dense(64, activation='relu')(inputs)
    hidden2 = Dense(32, activation='relu')(hidden1)
    output = Dense(1, activation='sigmoid')(hidden2)

    model = Model(inputs=inputs, outputs=output)
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

    return model




# Wrap the Keras model using KerasClassifier
keras_model = KerasClassifier(build_fn=create_mlp_model, input_shape=X_train_keras.shape[1], epochs=10, batch_size=32, verbose=0)

# Create a parameter grid for grid search
param_grid_keras = {
    'optimizer': ['adam', 'rmsprop'],
     'batch_size': [32, 64],
     'epochs': [10, 20],
}

# Create a GridSearchCV object for Keras
grid_search_keras = GridSearchCV(estimator=keras_model, param_grid=param_grid_keras, scoring='roc_auc', cv=3)

# Fit the grid search on the preprocessed data for Keras
grid_result_keras = grid_search_keras.fit(X_train_keras, y_train_keras)

# Get the best model for Keras
best_model_keras = grid_result_keras.best_estimator_

# Evaluate the best Keras model on the test set
y_pred_keras = best_model_keras.predict(X_test_keras)
accuracy_keras = accuracy_score(y_test_keras, y_pred_keras)

# Display Keras results
print(f"\nBest Parameters (Keras): {grid_result_keras.best_params_}")
print(f"Accuracy on Test Set (Keras): {accuracy_keras}")

from sklearn.metrics import roc_auc_score

# Evaluate the best Keras model on the test set
y_pred_proba_keras = best_model_keras.predict_proba(X_test_keras)[:, 1]
auc_score_keras = roc_auc_score(y_test_keras, y_pred_proba_keras)
print(f"AUC Score on Test Set (Keras): {auc_score_keras}")

from tensorflow.keras.optimizers import Adam, RMSprop

# Create a new Keras model with the best parameters
retrained_model = create_mlp_model(input_shape=X_train_keras.shape[1])

# Set the best parameters to the retrained model
retrained_model.optimizer = Adam() if best_params_keras['optimizer'] == 'adam' else RMSprop()
retrained_model.batch_size = best_params_keras['batch_size']
retrained_model.epochs = best_params_keras['epochs']

# Retrain the model on the entire training dataset
retrained_model.fit(X_train_keras, y_train_keras, epochs=best_params_keras['epochs'], batch_size=best_params_keras['batch_size'], verbose=1)

# Evaluate the retrained model on the test set
y_pred_retrained = retrained_model.predict(X_test_keras)

#Threshold the predicted probabilities to obtain binary predictions
y_pred_binary = (y_pred_retrained > 0.5).astype(int)

# Calculate accuracy on the test set
accuracy_retrained = accuracy_score(y_test_keras, y_pred_binary)

# Display retrained model results
print("\nRetrained Model Results:")
print(f"Best Parameters (Keras): {best_params_keras}")
print(f"Accuracy on Test Set (Retrained Model): {accuracy_retrained}")

# Make predictions on the training set
y_train_pred_keras = retrained_model.predict(X_train_keras)

# Threshold the predicted probabilities to obtain binary predictions
y_train_pred_binary = (y_train_pred_keras > 0.5).astype(int)

# Evaluate the model on the training set
accuracy_train_keras = accuracy_score(y_train_keras, y_train_pred_binary)
print(f"Accuracy on the Training Set (Retrained Keras Model): {accuracy_train_keras}")

# Make predictions on the testing set
y_test_pred_keras = retrained_model.predict(X_test_keras)

# Threshold the predicted probabilities to obtain binary predictions
y_test_pred_binary = (y_test_pred_keras > 0.5).astype(int)

# Evaluate the model on the testing set
accuracy_test_keras = accuracy_score(y_test_keras, y_test_pred_binary)
print(f"Accuracy on the Testing Set (Retrained Keras Model): {accuracy_test_keras}")

y_test_pred_rf

import pickle


# Define the filename for the pickle file
filename = '/content/drive/My Drive/Customer Churn/CustomerChurn_model.pkl'

# Save the best model to a pickle file
with open(filename, 'wb') as file:
    pickle.dump(best_model_keras, file)

print(f"Best model saved to {filename}")

import pickle


# Define the filename for the pickle file
filename = '/content/drive/My Drive/Customer Churn/scaler.pkl'

# Save the best model to a pickle file
with open(filename, 'wb') as file:
    pickle.dump(scaler_keras, file)

print(f"Best model saved to {filename}")